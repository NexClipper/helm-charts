# Values for configuring the deployment of TimescaleDB
# The charts README is at:
#    https://github.com/timescale/timescaledb-kubernetes/tree/master/charts/timescaledb-single
# Check out the various configuration options (administration guide) at:
#    https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/admin-guide.md
cli: false
timescaledb-single:
  # disable the chart if an existing TimescaleDB instance is used
  enabled: true
  image:
    repository: public.ecr.aws/nexclipper/timescaledb-ha
    tag: pg12-ts1.7-latest
  # create only a ClusterIP service
  loadBalancer:
    enabled: false
  # number or TimescaleDB pods to spawn (default is 3, 1 for no HA)
  replicaCount: 1
  persistentVolumes:
    data:
      size: 10Gi
    wal:
      size: 10Gi

# Values for configuring the deployment of the Promscale Connector
# The charts README is at:
#   https://github.com/timescale/promscale/tree/master/helm-chart
promscale:
  enabled: true
  image: public.ecr.aws/nexclipper/promscale:0.2.0
  # connection options
  connection:
    # the db name in which the metrics will be stored
    dbName: &metricDB postgres
    # user to connect to TimescaleDB with
    user: postgres
    password:
      # Name of a secret object (templated) containing the connection password
      # Key must be value of promscale.connection.user
      # Created by default if timescaledb-single.enabled=true
      secretTemplate: &dbPassSecret "{{ .Release.Name }}-timescaledb-passwords"
    host:
      # Host name (templated) of the database instance, default
      # to service created in timescaledb-single
      nameTemplate: &dbHost "{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local"
    port: 5432

  # configuration options for the service exposed by promscale
  service:
    # we disable the load balancer by default, only a ClusterIP service
    # will get created
    loadBalancer:
      enabled: false
    nodePort:
      enabled: true
      port: 30000 
  resources:
    requests:
      # By default this should be enough for a cluster
      # with only a few pods
      memory: 1Gi
      cpu: 1
# Values for configuring the deployment of Prometheus
# The Prometheus chart from the prometheus-community is used
# the guide for it can be found at:
#   https://artifacthub.io/packages/helm/prometheus-community/prometheus
# with the default values documented at:
#   https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
prometheus:
  enabled: true
  nodeExporter:
    image:
      repository: quay.io/prometheus/node-exporter
      tag: v1.0.1
  alertmanager:
    enabled: true
    image:
      repository: quay.io/prometheus/alertmanager
      tag: v0.21.0
  pushgateway:
    image:
      repository: quay.io/prometheus/pushgateway
      tag: v1.2.0
    enabled: true
  configmapReload:
    prometheus:
      image:
        repository: public.ecr.aws/nexclipper/configmap-reload
        tag: v0.4.0
  server:
    image:
      repository: quay.io/prometheus/prometheus
      tag: v2.21.0
    # we provide a config map which will have the
    # promscale connector already configured as
    # a remote endpoint for read/write (if enabled)
    # this overrides the config map the prometheus chart
    # creates
    global:
      external_labels:
        cluster_name: nexglobal
    configMapOverrideName: "prometheus-config"
    # values to be used in the override config map
    # for a Promscale remote_write and remote_read config
    timescaleRemote:
      # templated
      host: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local"
      protocol: http
      port: "9201"
      write:
        enabled: true
        # complete url = {protocol}://{host}:{port}/{endpoint}
        endpoint: "write"
        # write queue config
        # see: https://prometheus.io/docs/practices/remote_write/
        queue:
          max_shards: 30
      read:
        enabled: true
        # complete url = {protocol}://{host}:{port}/endpoint
        endpoint: "read"
  #You can add other configs from https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml here
  #For example, adding additional scrape jobs
  #extraScrapeConfigs:

# Values for configuring the deployment of Grafana
# The Grafana Community chart is used and the guide for it
# can be found at:
#   https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md
grafana:
  image:
    repository: public.ecr.aws/nexclipper/grafana
    tag: 7.3.1
  sidecar:
    image:
      repository: public.ecr.aws/nexclipper/k8s-sidecar
      tag: 1.1.0
    datasources:
      enabled: true
    dashboards:
      enabled: true
      files:
        - dashboards/k8s-cluster.json
        - dashboards/k8s-hardware.json
        - dashboards/k8s-cluster-kr.json
  envFromSecret: "{{ .Release.Name }}-grafana-db"
  initChownData:
    image:
      repository: gcr.io/nexclipper/busybox
      tag: "1.31.1"
  prometheus:
    datasource:
      enabled: true
      # By default url of data source is set to ts-prom connector instance
      # deployed with this chart. If a connector isn't used this should be
      # set to the prometheus-server.
      url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local:9201"
  timescale:
    database:
      enabled: true
      host:  *dbHost
      port: 5432
      user: grafanadb
      pass: grafanadb
      dbName: *metricDB
      schema: grafanadb
      sslMode: require
    datasource:
      enabled: true
      user: grafana
      pass: grafana
      dbName: *metricDB
      sslMode: require
      # By default the url/host is set to the db instance deployed
      # with this chart
      host: *dbHost
      port: 5432
    adminUser: postgres
    adminPassSecret: *dbPassSecret

#Enable PromLens  https://promlens.com/
#PromLens is a PromQL query builder, analyzer, and visualizer
promlens:
  enabled: true
  image: "public.ecr.aws/nexclipper/promlens:latest"
  # This default URL assumes access via port-forwarding to the connector. If using
  # a load balancer for the connector, change as appropriate
  # NOTE: the internal cluster address does not work here as requests are made by the browser.
  defaultPrometheusUrl:  "http://localhost:9201"
  licenseKey: ""
  license:
    enabled: false
  loadBalancer:
    enabled: false


